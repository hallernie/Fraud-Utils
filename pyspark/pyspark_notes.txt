**** (SQL) Logical order of operations
  FROM - Chooses table to get the base data records
  JOIN - Optains matching data records from other tables
  WHERE - Filters the base data
  GROUP BY - Aggregates the base data
  HAVING - Filters the aggregated data
  SELECT - Returns the final data
  ORDER BY - Sorts the final data



**** (PySpark) Starting PySpark via iPython (Recommended)
a. PySpark requires java 8. Set the JAVA_HOME environment variable

% export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_311.jdk/Contents/Home

b. Start ipython

% ipython

c. Enter the following in the iPython REPL

In [1]: from pyspark.sql import SparkSession

e. Instantiate a SparkSession

In [2]: spark = SparkSession.builder.getOrCreate()



**** (PySpark) Misc

a. Read .csv file, keeping header row

In [1]: dataframe_from_csv_file = spark.read.option("header",True).csv("./events.csv")


**** (PySpark) Get data frame information

*** *** Assume the sample pyspark data frame below and the import statement

import pyspark.sql.functions as F

In [122]: f1.printSchema()
root
 |-- Event Time: string (nullable = true)
 |-- Request ID: string (nullable = true)
 |-- Event Type: string (nullable = true)
 |-- Request Result: string (nullable = true)
 |-- Error Detail: string (nullable = true)
 |-- Policy: string (nullable = true)
 |-- Unknown Session: string (nullable = true)
 |-- Agent Type: string (nullable = true)
 |-- API Type: string (nullable = true)
 |-- Exact ID: string (nullable = true)
 |-- Session ID: string (nullable = true)
 |-- Review Status: string (nullable = true)
 |-- Policy Score: string (nullable = true)
 |-- Browser: string (nullable = true)
 |-- Browser Language: string (nullable = true)
 |-- Browser String: string (nullable = true)
 |-- Browser Version: string (nullable = true)
 |-- Cookies Enabled: string (nullable = true)
 |-- Javascript Enabled: string (nullable = true)
 |-- UA Browser: string (nullable = true)
 |-- UA Browser Alternative: string (nullable = true)
 |-- UA Browser Version Alternative: string (nullable = true)


*** *** Info as a list of tuples
print(f1.dtypes)

*** *** Get data frame column names as a list
f1.columns

*** *** Info on data contained in the data frame
f1.show()
f1.show(10)
f1.show(n=10, truncate=False)

*** *** Select some columns into a new data frame
f1.select("Review Status","Policy Score")
f1.select(F.col("Event Time"))
f1.select(F.col("Event Time"),F.col("Policy Score"))
f1.select(*[F.col("Review Status"),F.col("Policy Score")])
f1.select(["Review Status","Policy Score"])
f1.select(*["Review Status","Policy Score"])
f1.select(f1["Review Status"],f1["Policy Score"])
f1.select([f1["Review Status"],f1["Policy Score"]])
f1["Review Status","Policy Score"]

*** *** Select one column from a data frame into a Column object
f1["Review Status"]

*** *** Getting rid of columns
f1.drop("Review Status","Policy Score")
f1.drop(F.col("Review Status"),F.col("Policy Score"))
f1.drop(*["Review Status","Policy Score"])

*** *** Execute a pyspark function on a column and return the column as a data frame
f1.select(F.split(F.col("Event Time")," "))

--Or do the same, but give the resultant column and better name
f1.select(F.split(F.col("Event Time")," ").alias("line"))

*** *** Execute a pyspark function on a column and return a column object
F.split(f1["Event Time"]," ")

--Or do the same, but give the column a better name
F.split(f1["Event Time"]," ").alias("line")

*** *** Rename a column
f1.withColumnRenamed("Event Time","bubba")

*** *** Filtering rows
f1.where(F.col("Policy Score") == -35)
or
f1.filter(F.col("Policy Score") == -35)

*** *** Count #rows in a dataframe
f1.count()

*** *** Group data (returns pyspark.sql.group.GroupedData)
f1.groupBy(F.col("Review Status"))

--Count the elements in each group (returns a pyspark data frame)
f1.groupBy(F.col("Review Status")).count().show()

*** *** Order the data frame
f1.groupBy(F.col("Review Status")).count().orderBy(F.col("count").desc()).show()
or
f1.groupBy(F.col("Review Status")).count().orderBy(F.col("count"),ascending=False).show()

*** *** Write data frame to just one .csv file
f1.coalesce(1).write.csv("name.csv")

*** *** Add column to an existing data frame
f1.withColumn("date",F.col("Event Time").substr(1,10)).printSchema()

root
 |-- Event Time: string (nullable = true)
 |-- Request ID: string (nullable = true)
 |-- Event Type: string (nullable = true)
 |-- Request Result: string (nullable = true)
 |-- Error Detail: string (nullable = true)
 |-- Policy: string (nullable = true)
 |-- Unknown Session: string (nullable = true)
 |-- Agent Type: string (nullable = true)
 |-- API Type: string (nullable = true)
 |-- Exact ID: string (nullable = true)
 |-- Session ID: string (nullable = true)
 |-- Review Status: string (nullable = true)
 |-- Policy Score: string (nullable = true)
 |-- Browser: string (nullable = true)
 |-- Browser Language: string (nullable = true)
 |-- Browser String: string (nullable = true)
 |-- Browser Version: string (nullable = true)
 |-- Cookies Enabled: string (nullable = true)
 |-- Javascript Enabled: string (nullable = true)
 |-- UA Browser: string (nullable = true)
 |-- UA Browser Alternative: string (nullable = true)
 |-- UA Browser Version Alternative: string (nullable = true)
 |-- date: string (nullable = true)


*** *** Rename a column
f1.withColumnRenamed("Event Time", "Time")

*** *** Sort the columns
f1.select(sorted(f1.columns))

*** *** Alias my the name of my data frame
f1.alias("bob")

--Do the same thing, but then select a column using my aliased data frame name
f1.alias("bob").select(F.col("bob.Event Time"))


*** *** Joining tables
f1.join(f2, on="Request ID")
f1.join(f2, "Request ID")
f1.join(f2,on="Request ID", how="right")
f1.join(f2, f1["Request ID"] == f2["Request ID"])   # column "Request ID" appears twice in resultant data frame

# column "Request ID" appears twice in resultant data frame, but I can select the "left" table column
# using its column alias name
f1.alias("left").join(f2, f1["Request ID"] == f2["Request ID"]).select(F.col("left.Request ID"))


*** *** Creating a UDF (User Defined Function)

--Create a function

def convertCase(str):
	return str.upper()

-- Convert the function to a Pyspark sql function

convertUDF = F.udf(lambda z: convertCase(z))

--Now I can use the function like a regular Pyspark sql function

f2.select(convertUDF(F.col("Some Data")))
